{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d636a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import optimize_features\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9892f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    documents_f = open(filepath, 'rb')\n",
    "    file = pickle.load(documents_f)\n",
    "    documents_f.close()\n",
    "    \n",
    "    return file\n",
    "\n",
    "def save_pickle(data, filepath):\n",
    "    save_documents = open(filepath, 'wb')\n",
    "    pickle.dump(data, save_documents)\n",
    "    save_documents.close()\n",
    "    \n",
    "def test_classifier(X_train, y_train, X_test, y_test, classifier):\n",
    "    \n",
    "    print('#' * 50)\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    print(\"Testing \" + classifier_name)\n",
    "    now = time()\n",
    "    list_of_labels = sorted(list(set(y_train)))\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    print(\"Learing time {0}s\".format(time() - now))\n",
    "    now = time()\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"Predicting time {0}s\".format(time() - now))\n",
    "\n",
    "    precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    print(\"=================== Results ===================\")\n",
    "    print(\"            Fact     Opinion                   \")\n",
    "    print(\"F1       \" + str(f1))\n",
    "    print(\"Precision\" + str(precision))\n",
    "    print(\"Recall   \" + str(recall))\n",
    "    print(\"Accuracy \" + str(accuracy))\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    return model, precision, recall, accuracy, f1\n",
    "\n",
    "\n",
    "def cv(classifier, X_train, y_train):\n",
    "    scoring = ['accuracy', 'f1', 'f1_micro', 'f1_macro', 'precision', 'recall']\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    now = time()\n",
    "    print(\"Crossvalidating \" + classifier_name + \"...\")\n",
    "    cv_results = [cross_validate(classifier, X_train, y_train, cv=8, n_jobs=-1, scoring=scoring, return_train_score=False)]\n",
    "    print(\"Crosvalidation completed in {0}s\".format(time() - now))\n",
    "    #print(\"Accuracy: \" + str(accuracy[0]))\n",
    "    #print(\"Average accuracy: \" + str(np.array(accuracy[0]).mean()))\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "def print_cm(cm):\n",
    "    print(\"               Predicted Fact     Predicted Opinion      Total \")\n",
    "    print(\"Actual Fact:       \", cm[0][0], \"                  \", cm[0][1], \"           \", (cm[0][0] + cm[0][1]))\n",
    "    print(\"Actual Optinion:    \", cm[1][0], \"                 \",cm[1][1], \"         \", (cm[1][0] + cm[1][1]))\n",
    "    print(\"                   \", (cm[0][0] + cm[1][0]), \"                 \", (cm[0][1] + cm[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7038c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "#==============================================================================\n",
    "is_train = np.random.uniform(0, 1, len(data)) <= 0.85\n",
    "for index, example in enumerate(data):\n",
    "    data[index]['is_train'] = is_train[index]\n",
    "save_pickle(data, 'opinion_fact_sentences.pickle')\n",
    "\n",
    "data = load_pickle('opinion_fact_sentences.pickle')\n",
    "data = pd.DataFrame(data)\n",
    "data.head()\n",
    "\n",
    "count_opinion = 0\n",
    "count_fact = 0\n",
    "for example in data['y_label']:\n",
    "    if example == 0:\n",
    "        count_fact += 1\n",
    "    else:\n",
    "        count_opinion += 1\n",
    "print('Got {} opinions and {} factual sentences.'.format(count_opinion, count_fact))\n",
    "print('Total sentences: {}'.format(count_opinion + count_fact))\n",
    "\n",
    "# Divide the dataset into train and test sets\n",
    "train, test = data[data['is_train']==True], data[data['is_train']==False]\n",
    "print('Number of observations in the training data:', len(train))\n",
    "print('Number of observations in the test data:', len(test))\n",
    "\n",
    "# Feature selection and optimization\n",
    "# 0 - use all features\n",
    "# 1 - only dependency tags\n",
    "# 2 - only entity tags\n",
    "# 3 - only part of speech tags\n",
    "# 4 - top features from every label, as returned by random forest\n",
    "# 5 - keep only entities and POS, initial idea\n",
    "pick_features = 5\n",
    "features = optimize_features.test_features(data, pick_features)\n",
    "    \n",
    "y_train = pd.factorize(train['y_label'])[0]\n",
    "assert(len(y_train) + len(test) == len(data))\n",
    "\n",
    "real_values = test['y_label']\n",
    "save_pickle(real_values, './real_test_values.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the RANDOM FOREST classifier\n",
    "#==============================================================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from treeinterpreter import treeinterpreter\n",
    "import operator\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=500, oob_score=True, n_jobs=-1,random_state=50, \n",
    "                             max_features=None)\n",
    "rf_classifier, precision, recall, accuracy, f1 = test_classifier(train[features], y_train, test[features], test['y_label'], rf_classifier)\n",
    "rf_cv_scores = cv(rf_classifier, data[features], data['y_label'])\n",
    "\n",
    "feature_importance = list(zip(train[features], rf_classifier.feature_importances_))\n",
    "feature_importance = sorted(feature_importance, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(feature_importance[:10])\n",
    "\n",
    "prediction, bias, contributions = treeinterpreter.predict(rf_classifier, test.iloc[1:2][features])\n",
    "print(\"Prediction\", prediction)\n",
    "print(\"Bias (trainset prior)\", bias)\n",
    "print(\"Feature contributions:\")\n",
    "for contrib, feature in zip(contributions[0], test[features].columns):\n",
    "    print(feature, contrib)\n",
    "\n",
    "rf_classifier = rf_classifier.fit(train[features], train['y_label'])\n",
    "rf_preds_epos = rf_classifier.predict(test[features])\n",
    "rf_cm = metrics.confusion_matrix(test['y_label'], rf_preds_epos)\n",
    "print_cm(rf_cm)\n",
    "\n",
    "#                Predicted Fact     Predicted Opinion      Total \n",
    "# Actual Fact:        1007                    114             1121\n",
    "# Actual Optinion:     102                   1008           1110\n",
    "#                     1109                   1122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85fa32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE BAYES\n",
    "#==============================================================================\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_classifier, precision, recall, accuracy, f1 = test_classifier(train[features], y_train, test[features], test['y_label'], BernoulliNB())\n",
    "nb_cv_scores = cv(BernoulliNB(), data[features], data['y_label'])\n",
    "\n",
    "nb_classifier = BernoulliNB().fit(train[features], train['y_label'])\n",
    "nb_preds_epos = nb_classifier.predict(test[features])    \n",
    "nb_cm = metrics.confusion_matrix(test['y_label'], nb_preds_epos)\n",
    "print_cm(nb_cm )\n",
    "\n",
    "#                Predicted Fact     Predicted Opinion      Total \n",
    "# Actual Fact:        895                    226             1121\n",
    "# Actual Optinion:     77                   1033           1110\n",
    "#                     972                   1259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUPPORT VECTOR MACHINE\n",
    "#==============================================================================\n",
    "from sklearn import svm\n",
    "\n",
    "svm_classifier = svm.SVC()\n",
    "svm_classifier, precision, recall, accuracy, f1 = test_classifier(train[features], y_train, test[features], test['y_label'], svm_classifier)\n",
    "svm_acc = cv(svm_classifier, data[features], data['y_label'])\n",
    "\n",
    "svm_preds = svm_classifier.predict(test[features])\n",
    "print('Accuracy Score with svm:')\n",
    "print(metrics.accuracy_score(test['y_label'], svm_preds))\n",
    "# ~0.91 - 0.93 accuracy\n",
    "\n",
    "svm_classifier = svm_classifier.fit(train[features], train['y_label'])\n",
    "svm_preds_epos = svm_classifier.predict(test[features])    \n",
    "svm_cm = metrics.confusion_matrix(test['y_label'], svm_preds_epos)\n",
    "print_cm(svm_cm)\n",
    "\n",
    "#                Predicted Fact     Predicted Opinion      Total \n",
    "# Actual Fact:        1000                  121            1121\n",
    "# Actual Optinion:     65                   1045           1110\n",
    "#                     1065                  1166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57606d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION\n",
    "#==============================================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "feature_scaling = 1\n",
    "if feature_scaling:\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = train.copy()\n",
    "    train_scaled[features] = scaler.fit_transform(train_scaled[features])\n",
    "    test_scaled = test.copy()\n",
    "    test_scaled[features] = scaler.transform(test_scaled[features])\n",
    "    \n",
    "    data_scaled = data.copy()\n",
    "    data_scaled[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "    lr_classifier = LogisticRegression(random_state=0, verbose=1, C=0.01)\n",
    "    \n",
    "    lr_classifier_scaled, precision, recall, accuracy, f1 = test_classifier(train_scaled[features], y_train, \n",
    "                                                                     test_scaled[features], test['y_label'], \n",
    "                                                                     lr_classifier)\n",
    "    lr_acc_scaled = cv(lr_classifier, data_scaled[features], data_scaled['y_label'])\n",
    "\n",
    "\n",
    "\n",
    "print('Accuracy with logistic regression:')\n",
    "print(metrics.accuracy_score(test['y_label'], lr_pred))\n",
    "# Around 0.912 accuracy or 0.807 with feature scaling\n",
    "\n",
    "lr_classifier_scaled = lr_classifier.fit(train_scaled[features], train_scaled['y_label'])\n",
    "lr_preds_epos = lr_classifier_scaled.predict(test_scaled[features])    \n",
    "lr_cm = metrics.confusion_matrix(test['y_label'], lr_preds_epos)\n",
    "print_cm(lr_cm)\n",
    "\n",
    "#                 Predicted Fact     Predicted Opinion      Total \n",
    "# Actual Fact:        854                    267             1121\n",
    "# Actual Optinion:     93                   1017           1110\n",
    "#                     947                   1284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NET CLASSIFIER\n",
    "#==============================================================================\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "if feature_scaling:\n",
    "    nn_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(500, 5), random_state=1)\n",
    "    nn_classifier_scaled, precision, recall, accuracy, f1 = test_classifier(train_scaled[features], y_train, \n",
    "                                                                     test_scaled[features], test['y_label'], \n",
    "                                                                     nn_classifier)\n",
    "    nn_acc_scaled = cv(nn_classifier, data_scaled[features], data_scaled['y_label'])\n",
    "    \n",
    "    \n",
    "print('Accuracy Score with neural net:')\n",
    "print(metrics.accuracy_score(test['y_label'], nn_preds))\n",
    "# 0.921 Accuracy or around 0.920 with feature scaling (up to 0.93 with dependency features)\n",
    "\n",
    "nn_classifier_scaled = nn_classifier.fit(train_scaled[features], train_scaled['y_label'])\n",
    "nn_preds_epos = nn_classifier_scaled.predict(test_scaled[features])    \n",
    "nn_cm = metrics.confusion_matrix(test['y_label'], nn_preds_epos)\n",
    "print_cm(nn_cm)\n",
    "\n",
    "#                Predicted Fact     Predicted Opinion      Total \n",
    "# Actual Fact:        1019                  102             1121\n",
    "# Actual Optinion:     66                   1044           1110\n",
    "#                     1085                  1146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2422981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODELS\n",
    "#==============================================================================\n",
    "save_pickle(rf_classifier, 'epos_pickles/rf_classifier.pickle')\n",
    "save_pickle(svm_classifier, 'epos_pickles/svm_classifier.pickle')\n",
    "#save_pickle(lr_classifier, 'models/lr_classifier.pickle')\n",
    "save_pickle(lr_classifier_scaled, 'epos_pickles/lr_classifier_scaled.pickle')\n",
    "save_pickle(nn_classifier_scaled, 'epos_pickles/nn_classifier_scaled.pickle')\n",
    "#save_pickle(nn_classifier, 'models/nn_classifier.pickle')\n",
    "save_pickle(scaler, 'epos_pickles/scaler.pickle')\n",
    "\n",
    "save_pickle(nb_preds_epos, 'epos_pickles/nb_preds_epos.pickle')\n",
    "save_pickle(rf_preds_epos, 'epos_pickles/rf_preds_epos.pickle')\n",
    "save_pickle(lr_preds_epos, 'epos_pickles/lr_preds_epos.pickle')\n",
    "save_pickle(nn_preds_epos, 'epos_pickles/nn_preds_epos.pickle')\n",
    "save_pickle(svm_preds_epos, 'epos_pickles/svm_preds_epos.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedc3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af882998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd131c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0915bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3db48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cf4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
